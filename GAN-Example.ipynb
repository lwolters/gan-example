{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Product Description Generation\n",
    "\n",
    "## Install Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting numpy==1.18.3\n",
      "  Using cached numpy-1.18.3-cp38-cp38-manylinux1_x86_64.whl (20.6 MB)\n",
      "Collecting pandas==1.0.3\n",
      "  Using cached pandas-1.0.3-cp38-cp38-manylinux1_x86_64.whl (10.0 MB)\n",
      "Collecting torch==1.5.1\n",
      "  Downloading torch-1.5.1-cp38-cp38-manylinux1_x86_64.whl (753.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 753.2 MB 31 kB/s s eta 0:00:01     |████████████████                | 378.4 MB 12.3 MB/s eta 0:00:31     |████████████████▏               | 381.2 MB 12.3 MB/s eta 0:00:31     |█████████████████████████▏      | 593.0 MB 8.7 MB/s eta 0:00:19██████▏    | 638.2 MB 5.4 MB/s eta 0:00:22     |█████████████████████████████▉  | 702.1 MB 13.6 MB/s eta 0:00:04\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement torchvision==0.4.0 (from -r ../requirements.txt (line 4)) (from versions: 0.1.6, 0.1.7, 0.1.8, 0.1.9, 0.2.0, 0.2.1, 0.2.2, 0.2.2.post2, 0.2.2.post3, 0.5.0, 0.6.0, 0.6.1, 0.7.0)\u001b[0m\n",
      "\u001b[31mERROR: No matching distribution found for torchvision==0.4.0 (from -r ../requirements.txt (line 4))\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization\n",
    "### Load packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Load helper files\n",
    "sys.path.append(os.path.abspath('../'))\n",
    "import helper\n",
    "import network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/leonard/Workspace/development/research/product-attribute-extractor/pae-env/lib/python3.6/site-packages/IPython/core/interactiveshell.py:3063: DtypeWarning: Columns (0,1,2,3,4,5,6,7,8,9,11,12,15,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('asics-data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ID',\n",
       " 'Name',\n",
       " 'Size (EU)',\n",
       " 'Size (EU).1',\n",
       " 'Size (EU).2',\n",
       " 'Size (UK)',\n",
       " 'Size (US)',\n",
       " 'Size (JP)',\n",
       " 'Shoe Width',\n",
       " 'Color Code',\n",
       " 'Brand',\n",
       " 'Gender',\n",
       " 'Season',\n",
       " 'SAP Material',\n",
       " 'Product Division',\n",
       " 'Style Name (Dutch)',\n",
       " 'Style Name',\n",
       " 'Style Name (French)',\n",
       " 'Style Name (German)',\n",
       " 'Style Name (Italian)',\n",
       " 'Style Name (Spanish)',\n",
       " 'Color Name',\n",
       " 'Color Name (Dutch)',\n",
       " 'Color Name (French)',\n",
       " 'Color Name (German)',\n",
       " 'Color Name (Italian)',\n",
       " 'Color Name (Spanish)',\n",
       " 'Long Description (Dutch)',\n",
       " 'Long Description',\n",
       " 'Long Description (French)',\n",
       " 'Long Description (German)',\n",
       " 'Long Description (Italian)',\n",
       " 'Long Description (Spanish)',\n",
       " 'Page Description (Dutch)',\n",
       " 'Page Description',\n",
       " 'Page Description (French)',\n",
       " 'Page Description (German)',\n",
       " 'Page Description (Italian)',\n",
       " 'Page Description (Spanish)',\n",
       " 'Short Description (Dutch)',\n",
       " 'Short Description',\n",
       " 'Short Description (French)',\n",
       " 'Short Description (German)',\n",
       " 'Short Description (Italian)',\n",
       " 'Short Description (Spanish)',\n",
       " 'List Price - AT (EUR)',\n",
       " 'List Price - BE (EUR)',\n",
       " 'List Price - DE (EUR)',\n",
       " 'List Price - ES (EUR)',\n",
       " 'List Price - FR (EUR)',\n",
       " 'List Price - GB (GBP)',\n",
       " 'List Price - IE (EUR)',\n",
       " 'List Price - IT (EUR)',\n",
       " 'List Price - NL (EUR)',\n",
       " 'Sale Price - AT (EUR)',\n",
       " 'Sale Price - BE (EUR)',\n",
       " 'Sale Price - DE (EUR)',\n",
       " 'Sale Price - ES (EUR)',\n",
       " 'Sale Price - FR (EUR)',\n",
       " 'Sale Price - GB (GBP)',\n",
       " 'Sale Price - IE (EUR)',\n",
       " 'Sale Price - IT (EUR)',\n",
       " 'Sale Price - NL (EUR)',\n",
       " 'Web Color Article Id']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df[['Short Description', 'Long Description', 'Page Description']].head\n",
    "\n",
    "#text = '\\n'.join(df[['Short Description', 'Long Description', 'Page Description']])\n",
    "#text = '\\n'.join(df['Long Description'].astype(str))\n",
    "#text = '\\n'.join(df[df['Long Description'].str.len() > 3])\n",
    "\n",
    "text = '\\n'.join(df.loc[df['Long Description'].str.len() > 3]['Long Description'].drop_duplicates())\n",
    "#text += '\\n'.join(df.loc[df['Short Description'].str.len() > 3]['Short Description'].drop_duplicates())\n",
    "#text += '\\n'.join(df.loc[df['Page Description'].str.len() > 3]['Page Description'].drop_duplicates())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Stats\n",
      "Roughly the number of unique words: 23458\n",
      "Number of lines: 10953\n",
      "Average number of words in each line: 40.30722176572628\n",
      "\n",
      "The lines 0 to 5:\n",
      "<p>Layer this bra with your gym kit favourites for a striking look that’s also comfortable. Feel flawless with Motion Dry technology to keep sweat away from your skin.</p>\n",
      "\n",
      "<p>The cutout back begs to be seen, so wear under low-backed or racer-back tops for a stylish touch. Feel fantastic in soft fabric that stretches to support you whether you’re hitting the HIITs or swinging kettlebells.</p>\n",
      "<ul>\n",
      "\t<li>Feel comfortable in soft, flexible fabric</li>\n"
     ]
    }
   ],
   "source": [
    "print('Dataset Stats')\n",
    "print('Roughly the number of unique words: {}'.format(len({word: None for word in text.split()})))\n",
    "\n",
    "lines = text.split('\\n')\n",
    "print('Number of lines: {}'.format(len(lines)))\n",
    "word_count_line = [len(line.split()) for line in lines]\n",
    "print('Average number of words in each line: {}'.format(np.average(word_count_line)))\n",
    "\n",
    "helper.view_lines(text, 0, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-process all the data and save it\n",
    "\n",
    "Running the code cell below will pre-process all the data and save it to file. You're encouraged to lok at the code for `preprocess_and_save_data` in the `helpers.py` file to see what it's doing in detail, but you do not need to change this code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The lines 0 to 5:\n",
      "Layer this bra with your gym kit favourites for a striking look that’s also comfortable. Feel flawless with Motion Dry technology to keep sweat away from your skin.\n",
      "\n",
      "The cutout back begs to be seen, so wear under low-backed or racer-back tops for a stylish touch. Feel fantastic in soft fabric that stretches to support you whether you’re hitting the HIITs or swinging kettlebells.\n",
      "\n",
      " Feel comfortable in soft, flexible fabric\n"
     ]
    }
   ],
   "source": [
    "# cleaning data\n",
    "\n",
    "# Remove tags\n",
    "#text = re.sub(\"&lt;/?.*?&gt;\",\" &lt;&gt; \", text)\n",
    "text = helper.clean_html(text)\n",
    "text = helper.strip_whitespaces(text)\n",
    "\n",
    "# Replace special characters\n",
    "text = helper.strip_accents(text)\n",
    "\n",
    "helper.view_lines(text, 0, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre-process training data\n",
    "helper.preprocess_and_save_data(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check Point\n",
    "If you ever decide to come back to this notebook or have to restart the notebook, you can start from here. The preprocessed data has been saved to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "int_text, vocab_to_int, int_to_vocab, token_dict = helper.load_preprocess()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the Neural Network\n",
    "In this section, you'll build the components necessary to build an RNN by implementing the RNN Module and forward and backpropagation functions.\n",
    "\n",
    "### Check Access to GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test dataloader\n",
    "#helper.test_batch_data(range(50))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters\n",
    "\n",
    "Set and train the neural network with the following parameters:\n",
    "- Set `sequence_length` to the length of a sequence.\n",
    "- Set `batch_size` to the batch size.\n",
    "- Set `num_epochs` to the number of epochs to train for.\n",
    "- Set `learning_rate` to the learning rate for an Adam optimizer.\n",
    "- Set `vocab_size` to the number of unique tokens in our vocabulary.\n",
    "- Set `output_size` to the desired size of the output.\n",
    "- Set `embedding_dim` to the embedding dimension; smaller than the vocab_size.\n",
    "- Set `hidden_dim` to the hidden dimension of your RNN.\n",
    "- Set `n_layers` to the number of layers/cells in your RNN.\n",
    "- Set `show_every_n_batches` to the number of batches at which the neural network should print progress.\n",
    "\n",
    "If the network isn't getting the desired results, tweak these parameters and/or the layers in the `RNN` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on: cuda\n"
     ]
    }
   ],
   "source": [
    "# Sequence Length\n",
    "sequence_length = 15  # of words in a sequence\n",
    "batch_size = 50\n",
    "\n",
    "# data loader - do not change\n",
    "train_loader = helper.batch_data(int_text, sequence_length, batch_size)\n",
    "\n",
    "# Training parameters\n",
    "num_epochs = 9\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Model parameters\n",
    "vocab_size = len(vocab_to_int)\n",
    "output_size = len(vocab_to_int) # output is one word of the complete dictionary\n",
    "embedding_dim = 450 # This should be OK, size vocab is not that long\n",
    "hidden_dim = 400\n",
    "n_layers = 2\n",
    "\n",
    "# Show stats for every n number of batches\n",
    "show_every_n_batches = 500\n",
    "\n",
    "# Training on GPU?\n",
    "train_on_gpu = torch.cuda.is_available()\n",
    "device = \"cuda\" if train_on_gpu else \"cpu\"\n",
    "print(\"Running on: {}\".format(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STOP HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training (cuda: True) for 9 epoch(s)...\n",
      "Epoch:  1\n",
      "Epoch:    1/9     Loss: 6.46771284866333\n"
     ]
    }
   ],
   "source": [
    "# create model and move to gpu if available\n",
    "rnn = network.RNN(vocab_size, output_size, embedding_dim, hidden_dim, n_layers, train_on_gpu, dropout=0.5)\n",
    "if rnn.train_on_gpu:\n",
    "    rnn.cuda()\n",
    "\n",
    "# defining loss and optimization functions for training\n",
    "optimizer = torch.optim.Adam(rnn.parameters(), lr=learning_rate)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# training the model\n",
    "trained_rnn = network.train_rnn(rnn, batch_size, optimizer, criterion, num_epochs, train_loader, \\\n",
    "                                train_on_gpu, show_every_n_batches)\n",
    "\n",
    "# saving the trained model\n",
    "helper.save_model('trained_rnn', trained_rnn)\n",
    "print('Model Trained and Saved')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Checkpoint\n",
    "\n",
    "After running the above training cell, your model will be saved by name, `trained_rnn`, and if you save your notebook progress, **you can pause here and come back to this code at another time**. You can resume your progress by running the next cell, which will load in our word:id dictionaries _and_ load in your saved model by name!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, vocab_to_int, int_to_vocab, token_dict = helper.load_preprocess()\n",
    "trained_rnn = helper.load_model('trained_rnn')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Product Description\n",
    "With the network trained and saved, you'll use it to generate a new, \"fake\" product descriptions.\n",
    "\n",
    "### Generate Text\n",
    "To generate the text, the network needs to start with a single word and repeat its predictions until it reaches a set length. You'll be using the `generate` function to do this. It takes a word id to start with, `prime_id`, and generates a set length of text, `predict_len`. Also note that it uses topk sampling to introduce some randomness in choosing the most likely next word, given an output set of word scores!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the cell multiple times to get different results!\n",
    "gen_length = 100 # modify the length to your preference\n",
    "prime_word = 'run' # name for starting the script\n",
    "\n",
    "pad_word = helper.SPECIAL_WORDS['PADDING']\n",
    "generated_script = network.generate(trained_rnn, vocab_to_int[prime_word], int_to_vocab, \\\n",
    "                                    token_dict, vocab_to_int[pad_word], sequence_length, \\\n",
    "                                    train_on_gpu, gen_length)\n",
    "print(generated_script)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save your favorite scripts\n",
    "\n",
    "Once you have a script that you like (or find interesting), save it to a text file!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save script to a text file\n",
    "f =  open(\"./generated/script_rood_joris_1.txt\",\"w\")\n",
    "f.write(generated_script)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pae-env",
   "language": "python",
   "name": "pae-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
